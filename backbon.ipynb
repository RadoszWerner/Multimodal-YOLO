{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "292f6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch\n",
    "from ultralytics.nn.modules import Concat, C2f, Conv\n",
    "\n",
    "\n",
    "pretrained_model = YOLO('yolov8m.pt').model\n",
    "backbone = nn.Sequential(*list(pretrained_model.model.children())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "207c9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBackbone(nn.Module):\n",
    "    def __init__(self, layers, out_idx=[2, 4, 9]):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.out_idx = out_idx\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if idx in self.out_idx:\n",
    "                outputs.append(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b13af2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 3, 3, 3])\n",
      "torch.Size([48, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "backbone_rgb = CustomBackbone(backbone)\n",
    "backbone_ir = copy.deepcopy(backbone_rgb)\n",
    "\n",
    "backbone_ir.layers[0].conv = nn.Conv2d(1, 48, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "print(backbone_rgb.layers[0].conv.weight.shape)  # Powinno być torch.Size([48, 3, 3, 3])\n",
    "print(backbone_ir.layers[0].conv.weight.shape)   # Powinno być torch.Size([48, 1, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddd905cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomNeck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Przykładowe bloki do przetwarzania map cech różnych skal:\n",
    "        # Przyjmujemy, że features[0] ma kształt [B, 96, 160, 160],\n",
    "        # features[1] ma kształt [B, 192, 80, 80] i\n",
    "        # features[2] ma kształt [B, 576, 20, 20].\n",
    "        \n",
    "        # Bloki dla najwyższej rozdzielczości\n",
    "        self.conv_P0 = nn.Sequential(\n",
    "            nn.Conv2d(96, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Bloki dla środkowej rozdzielczości\n",
    "        self.conv_P1 = nn.Sequential(\n",
    "            nn.Conv2d(192, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Bloki dla najniższej rozdzielczości\n",
    "        self.conv_P2 = nn.Sequential(\n",
    "            nn.Conv2d(576, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Następnie możesz scalić mapy cech – tutaj przykład scalenia po upsampling'u:\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv2d(128 + 256 + 512, 512, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Przykładowa głowica detekcji – liczba wyjść zależy od Twojej implementacji\n",
    "        self.detect = nn.Conv2d(512, 80, kernel_size=1)  # np. 80 może oznaczać np. klasy lub inne wyjście\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Załóżmy, że features to lista trzech map cech z backbone: [P0, P1, P2]\n",
    "        # P0: [B, 96, 160, 160]\n",
    "        # P1: [B, 192, 80, 80]\n",
    "        # P2: [B, 576, 20, 20]\n",
    "        \n",
    "        # Najpierw przetwarzamy każdą mapę cech osobno\n",
    "        p0 = self.conv_P0(features[0])\n",
    "        p1 = self.conv_P1(features[1])\n",
    "        p2 = self.conv_P2(features[2])\n",
    "        \n",
    "        # Następnie dopasowujemy rozmiary przestrzenne:\n",
    "        # Zakładamy, że chcemy scalić wszystkie mapy do rozmiaru najdrobniejszej mapy, ale\n",
    "        # często lepszym rozwiązaniem jest dopasowanie do najwyższej rozdzielczości,\n",
    "        # wtedy wykorzystując upsampling.\n",
    "        # Przykład: upsamplujemy p2 do rozmiaru p0:\n",
    "        p2_up = F.interpolate(p2, size=p0.shape[-2:], mode='nearest')\n",
    "        p1_up = F.interpolate(p1, size=p0.shape[-2:], mode='nearest')\n",
    "        \n",
    "        # Teraz łączymy je wzdłuż kanału\n",
    "        fused = torch.cat([p0, p1_up, p2_up], dim=1)  # wynik: [B, 128+256+512, 160, 160]\n",
    "        fused = self.fuse(fused)\n",
    "        \n",
    "        # Ostatecznie przekazujemy scalone cechy do głowicy detekcji\n",
    "        out = self.detect(fused)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cef00476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomYOLO(nn.Module):\n",
    "    def __init__(self, pretrained_model, backbone_rgb, backbone_ir):\n",
    "        super().__init__()\n",
    "        self.backbone_rgb = backbone_rgb\n",
    "        self.backbone_ir  = backbone_ir\n",
    "        \n",
    "        # Używamy własnego modułu neck, który przyjmuje listę cech\n",
    "        self.neck_head = CustomNeck()\n",
    "        \n",
    "    def forward(self, x_rgb, x_ir):\n",
    "        features_rgb = self.backbone_rgb(x_rgb)\n",
    "        features_ir  = self.backbone_ir(x_ir)\n",
    "        \n",
    "        # Przykładowa fuzja – możesz fuzować na wiele sposobów.\n",
    "        # Tutaj sumujemy odpowiadające się mapy z RGB i IR\n",
    "        fused_features = [f_rgb + f_ir for f_rgb, f_ir in zip(features_rgb, features_ir)]\n",
    "        \n",
    "        # Teraz przekazujemy listę scalonych map do neck/head\n",
    "        # Twój moduł CustomNeck wie już, które mapy co oznaczają\n",
    "        return self.neck_head(fused_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbc248f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicjalizacja modelu\n",
    "model = CustomYOLO(pretrained_model, backbone_rgb, backbone_ir)\n",
    "\n",
    "x_rgb = torch.randn(1, 3, 640, 640)\n",
    "x_ir = torch.randn(1, 1, 640, 640)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x_rgb, x_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "090642a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.9680e-03, -1.1005e-01, -6.4227e-01,  ..., -1.6373e-01, -5.4696e-02, -2.1569e-01],\n",
       "          [ 2.1275e-01, -1.1080e-01, -3.9340e-01,  ..., -1.8092e-01,  1.3008e-01, -2.9765e-01],\n",
       "          [-2.4596e-01, -7.0585e-02, -4.0877e-01,  ...,  5.7011e-01, -1.1018e-01, -3.1984e-01],\n",
       "          ...,\n",
       "          [-7.6895e-02,  1.1608e-01,  2.2254e-01,  ...,  1.0066e+00,  4.6405e-01, -3.6167e-02],\n",
       "          [ 2.1799e-01, -3.4284e-03,  1.7007e-01,  ...,  6.8007e-01,  5.7986e-01,  2.3436e-01],\n",
       "          [ 5.8811e-02,  3.7553e-02, -7.5816e-02,  ...,  5.9435e-01,  1.5673e-01,  2.6751e-02]],\n",
       "\n",
       "         [[-1.5786e-01, -7.3406e-02,  2.0242e-01,  ..., -5.4722e-01, -6.0957e-01, -2.4576e-01],\n",
       "          [-1.4729e-01, -4.5067e-01, -1.8061e-01,  ..., -7.8675e-01, -7.2588e-01, -5.8055e-02],\n",
       "          [ 8.6550e-02, -2.1193e-01, -4.8839e-01,  ..., -1.4798e-01, -4.4001e-01,  1.7295e-01],\n",
       "          ...,\n",
       "          [-2.8932e-01, -4.0438e-01,  2.3402e-01,  ..., -5.3718e-01, -4.8615e-01, -6.5216e-02],\n",
       "          [-2.2744e-01, -1.6853e-01, -7.0525e-02,  ..., -8.6864e-01, -1.1570e+00, -4.0693e-01],\n",
       "          [ 9.6616e-02, -1.3633e-01, -1.6328e-01,  ..., -8.8611e-01, -7.0462e-01, -6.9345e-01]],\n",
       "\n",
       "         [[ 5.7783e-01,  5.6654e-01,  6.4752e-01,  ...,  1.5626e-01,  8.5151e-01,  7.3584e-01],\n",
       "          [ 4.6093e-01,  2.9128e-01,  8.3122e-01,  ..., -1.5583e-01,  4.5859e-01,  5.5607e-01],\n",
       "          [ 4.9389e-01,  2.4247e-01,  8.8896e-01,  ..., -3.6395e-01,  7.5882e-01,  7.3874e-01],\n",
       "          ...,\n",
       "          [ 7.1687e-03,  3.9475e-01,  8.4528e-01,  ...,  3.7145e-01,  9.1697e-01,  6.4104e-01],\n",
       "          [ 2.2743e-01,  4.4847e-01,  2.6768e-01,  ..., -1.9048e-01, -2.6236e-02, -1.6034e-01],\n",
       "          [ 3.9796e-01,  4.4402e-01,  1.1137e-01,  ..., -3.7721e-02,  1.2627e-01, -3.1328e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.4444e-01, -1.7715e-01, -8.4764e-01,  ..., -3.1346e-01, -8.5218e-02, -7.9042e-02],\n",
       "          [-3.8860e-01,  1.7758e-02, -7.1540e-01,  ..., -3.6487e-01, -1.3702e-02, -1.1255e-01],\n",
       "          [-8.4255e-03,  1.2375e-02,  1.0492e-01,  ..., -6.6783e-02, -3.6320e-01, -3.4818e-01],\n",
       "          ...,\n",
       "          [ 5.6164e-01,  5.6458e-01, -8.1238e-01,  ...,  7.5751e-02, -3.0183e-01, -1.5524e-01],\n",
       "          [ 2.2418e-01, -2.6620e-02,  2.0053e-01,  ...,  4.8785e-01,  2.6184e-01, -1.9106e-01],\n",
       "          [ 9.2757e-02, -7.2307e-02,  2.0841e-01,  ...,  2.5969e-01,  1.2450e-01,  9.8999e-02]],\n",
       "\n",
       "         [[ 1.6657e-02,  5.3765e-02,  1.0543e+00,  ..., -3.3057e-01, -2.4290e-02, -1.5763e-01],\n",
       "          [ 4.8110e-01,  9.1658e-02,  1.0240e+00,  ..., -2.3958e-01,  8.9501e-02,  1.7481e-01],\n",
       "          [ 2.4537e-01,  4.4969e-01,  5.6183e-01,  ..., -6.5334e-01,  1.0751e-01,  3.9937e-01],\n",
       "          ...,\n",
       "          [ 6.6130e-01,  4.2802e-01, -1.2151e-01,  ...,  3.0640e-01, -4.2931e-02, -2.0297e-01],\n",
       "          [ 9.0920e-01,  5.8814e-01,  4.1169e-01,  ..., -2.2049e-01, -1.4729e-01,  3.2822e-02],\n",
       "          [ 1.0070e+00,  7.4701e-01, -1.5140e-02,  ..., -1.7249e-01, -9.2268e-02, -3.1742e-01]],\n",
       "\n",
       "         [[-1.2164e+00, -1.0533e+00, -1.6630e+00,  ..., -5.2271e-02,  1.1853e-03, -1.0625e-01],\n",
       "          [-1.1876e+00, -1.1214e+00, -1.5641e+00,  ..., -3.2992e-01,  1.4916e-01,  4.7353e-02],\n",
       "          [-1.2157e+00, -1.4471e+00, -1.6513e+00,  ..., -6.9616e-01, -8.8332e-01, -9.6829e-01],\n",
       "          ...,\n",
       "          [-6.7800e-01, -5.6897e-01, -1.2959e+00,  ..., -4.4166e-01, -8.6275e-01, -7.1684e-01],\n",
       "          [-7.4993e-02, -6.2670e-01, -5.5623e-01,  ..., -1.2043e+00, -9.4389e-01, -9.7753e-01],\n",
       "          [-2.6472e-01, -5.4418e-01, -5.1172e-01,  ..., -1.3102e+00, -9.6036e-01, -1.3557e+00]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20ee6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class LLVIPDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None, img_size=(640,640)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Ścieżka do katalogu LLVIP.\n",
    "            split (str): 'train' lub 'test'\n",
    "            transform: opcjonalne transformacje (np. albumentations) stosowane na obrazach.\n",
    "            img_size: rozmiar docelowy obrazów (może być potrzebny przy skalowaniu bounding boxów).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Katalogi z obrazami RGB i IR\n",
    "        self.rgb_dir = os.path.join(root_dir, 'visible', split)\n",
    "        self.ir_dir  = os.path.join(root_dir, 'infrared', split)\n",
    "        # Katalog z adnotacjami\n",
    "        self.ann_dir = os.path.join(root_dir, 'Annotations')\n",
    "        \n",
    "        # Pobieramy listę plików; załóżmy, że rozszerzenie to .jpg\n",
    "        self.filenames = [f for f in os.listdir(self.rgb_dir) if f.lower().endswith('.jpg')]\n",
    "        self.filenames.sort()  # Dla spójności\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def parse_voc_annotation(self, xml_file):\n",
    "        \"\"\"\n",
    "        Parsowanie adnotacji w formacie VOC z podanego pliku XML.\n",
    "        Zwraca słownik z:\n",
    "          'boxes': tensor o wymiarach [N, 4] (x_min, y_min, x_max, y_max),\n",
    "          'labels': tensor o wymiarach [N], (przyjmujemy, że etykiety są konwertowane do int)\n",
    "        \n",
    "        Jeśli masz mapowanie nazw klas na liczby, można je dodać.\n",
    "        \"\"\"\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for obj in root.findall('object'):\n",
    "            # Pobierz etykietę jako string\n",
    "            label = obj.find('name').text\n",
    "            # Przykład: przekonwertuj etykietę do liczby – tutaj zakładamy prostą konwersję lub słownik mapowania\n",
    "            # Możesz stworzyć własny słownik, np. {'person': 1, 'car': 2, ...}. Dla przykładu użyjemy hash,\n",
    "            # ale w praktyce lepiej jawnie zdefiniować mapowanie.\n",
    "            label_int = abs(hash(label)) % 1000  # Przykładowa konwersja\n",
    "            \n",
    "            bbox = obj.find('bndbox')\n",
    "            x_min = float(bbox.find('xmin').text)\n",
    "            y_min = float(bbox.find('ymin').text)\n",
    "            x_max = float(bbox.find('xmax').text)\n",
    "            y_max = float(bbox.find('ymax').text)\n",
    "            \n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(label_int)\n",
    "            \n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.zeros((0,4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            \n",
    "        return {'boxes': boxes, 'labels': labels}\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        \n",
    "        # Ścieżki do obrazów oraz adnotacji\n",
    "        rgb_path = os.path.join(self.rgb_dir, filename)\n",
    "        ir_path  = os.path.join(self.ir_dir, filename)\n",
    "        # Zakładamy, że plik XML ma taką samą nazwę, ale z rozszerzeniem .xml\n",
    "        xml_filename = os.path.splitext(filename)[0] + '.xml'\n",
    "        ann_path = os.path.join(self.ann_dir, xml_filename)\n",
    "        \n",
    "        # Wczytanie obrazów\n",
    "        rgb_img = cv2.imread(rgb_path)\n",
    "        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
    "        ir_img = cv2.imread(ir_path, cv2.IMREAD_GRAYSCALE)\n",
    "        ir_img = np.expand_dims(ir_img, axis=-1)  # [H, W, 1]\n",
    "\n",
    "        # Opcjonalna zmiana rozmiaru\n",
    "        rgb_img = cv2.resize(rgb_img, self.img_size)\n",
    "        ir_img = cv2.resize(ir_img, self.img_size)\n",
    "        \n",
    "        # Normalizacja\n",
    "        rgb_img = rgb_img.astype(np.float32) / 255.0\n",
    "        ir_img = ir_img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Konwersja na tensory: zmiana wymiarów: H, W, C -> C, H, W\n",
    "        rgb_img = torch.from_numpy(rgb_img).permute(2, 0, 1)\n",
    "        ir_img = torch.from_numpy(ir_img).permute(2, 0, 1)\n",
    "        \n",
    "        # Wczytanie adnotacji\n",
    "        target = self.parse_voc_annotation(ann_path)\n",
    "        \n",
    "        # Jeżeli używasz dodatkowych transformacji (np. augumentacji) to możesz je zastosować tu\n",
    "        if self.transform is not None:\n",
    "            # transform powinien obsługiwać dwa obrazy. Możesz zdefiniować własną funkcję, która będzie to obsługiwać.\n",
    "            augmented = self.transform(image=rgb_img.numpy(), mask=ir_img.numpy())\n",
    "            rgb_img = torch.from_numpy(augmented['image']).permute(2, 0, 1)\n",
    "            ir_img = torch.from_numpy(augmented['mask']).permute(2, 0, 1)\n",
    "        \n",
    "        return rgb_img, ir_img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f51de76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja collate_fn do DataLoadera (gdy targets mają zmienną liczbę elementów)\n",
    "def collate_fn(batch):\n",
    "    rgb_imgs, ir_imgs, targets = zip(*batch)\n",
    "    rgb_imgs = torch.stack(rgb_imgs, dim=0)\n",
    "    ir_imgs  = torch.stack(ir_imgs, dim=0)\n",
    "    return rgb_imgs, ir_imgs, list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02d5946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_loss(outputs, targets):\n",
    "    \"\"\"\n",
    "    Funkcja straty tylko do celów demonstracyjnych.\n",
    "    Zakładamy, że outputs to tensor wyjściowy modelu,\n",
    "    a targets to lista adnotacji – tutaj generujemy cel jako tensor zer.\n",
    "    \"\"\"\n",
    "    # Przykladowo celowy tensor o takim samym kształcie jak outputs\n",
    "    target_tensor = torch.zeros_like(outputs)\n",
    "    loss = F.mse_loss(outputs, target_tensor)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6101ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, num_epochs=10, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (x_rgb, x_ir, targets) in enumerate(train_loader):\n",
    "            x_rgb = x_rgb.to(device)\n",
    "            x_ir  = x_ir.to(device)\n",
    "            # Przenoszenie adnotacji (targets) na urządzenie\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_rgb, x_ir)\n",
    "            loss = dummy_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0741d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e909513",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Uruchomienie treningu\u001b[39;00m\n\u001b[32m     18\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, optimizer, num_epochs, device)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m      5\u001b[39m     epoch_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_rgb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_rgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_ir\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mLLVIPDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Konwersja na tensory: zmiana wymiarów: H, W, C -> C, H, W\u001b[39;00m\n\u001b[32m    101\u001b[39m rgb_img = torch.from_numpy(rgb_img).permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m ir_img = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mir_img\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Wczytanie adnotacji\u001b[39;00m\n\u001b[32m    105\u001b[39m target = \u001b[38;5;28mself\u001b[39m.parse_voc_annotation(ann_path)\n",
      "\u001b[31mRuntimeError\u001b[39m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "# Ścieżka do katalogu LLVIP – zmodyfikuj na właściwą ścieżkę\n",
    "root_dir = 'LLVIP'\n",
    "\n",
    "# Utworzenie datasetu i DataLoadera (dla treningu)\n",
    "dataset = LLVIPDataset(root_dir, split='train', img_size=(640, 640))\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Wczytanie pretrenowanego modelu YOLOv8m (upewnij się, że model 'yolov8m.pt' jest dostępny)\n",
    "pretrained_model = YOLO('yolov8m.pt')\n",
    "\n",
    "# Utworzenie customowego modelu\n",
    "model = CustomYOLO(pretrained_model, backbone_rgb, backbone_ir)\n",
    "\n",
    "# Konfiguracja optymalizatora\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Uruchomienie treningu\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, optimizer, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
