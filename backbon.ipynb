{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292f6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch\n",
    "from ultralytics.nn.modules import Concat, C2f, Conv\n",
    "\n",
    "\n",
    "pretrained_model = YOLO('yolov8m.pt').model\n",
    "backbone = nn.Sequential(*list(pretrained_model.model.children())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "207c9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBackbone(nn.Module):\n",
    "    def __init__(self, layers, out_idx=[2, 4, 9]):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.out_idx = out_idx\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if idx in self.out_idx:\n",
    "                outputs.append(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13af2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 3, 3, 3])\n",
      "torch.Size([48, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "backbone_rgb = CustomBackbone(backbone)\n",
    "backbone_ir = copy.deepcopy(backbone_rgb)\n",
    "\n",
    "backbone_ir.layers[0].conv = nn.Conv2d(1, 48, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "print(backbone_rgb.layers[0].conv.weight.shape)\n",
    "print(backbone_ir.layers[0].conv.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd905cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomNeck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Przykładowe bloki do przetwarzania map cech różnych skal:\n",
    "        # Przyjmujemy, że features[0] ma kształt [B, 96, 160, 160],\n",
    "        # features[1] ma kształt [B, 192, 80, 80] i\n",
    "        # features[2] ma kształt [B, 576, 20, 20].\n",
    "        \n",
    "        # Bloki dla najwyższej rozdzielczości\n",
    "        self.conv_P0 = nn.Sequential(\n",
    "            nn.Conv2d(96, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Bloki dla środkowej rozdzielczości\n",
    "        self.conv_P1 = nn.Sequential(\n",
    "            nn.Conv2d(192, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Bloki dla najniższej rozdzielczości\n",
    "        self.conv_P2 = nn.Sequential(\n",
    "            nn.Conv2d(576, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Następnie możesz scalić mapy cech – tutaj przykład scalenia po upsampling'u:\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv2d(128 + 256 + 512, 512, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Przykładowa głowica detekcji – liczba wyjść zależy od Twojej implementacji\n",
    "        self.detect = nn.Conv2d(512, 80, kernel_size=1)  # np. 80 może oznaczać np. klasy lub inne wyjście\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Załóżmy, że features to lista trzech map cech z backbone: [P0, P1, P2]\n",
    "        # P0: [B, 96, 160, 160]\n",
    "        # P1: [B, 192, 80, 80]\n",
    "        # P2: [B, 576, 20, 20]\n",
    "        \n",
    "        # Najpierw przetwarzamy każdą mapę cech osobno\n",
    "        p0 = self.conv_P0(features[0])\n",
    "        p1 = self.conv_P1(features[1])\n",
    "        p2 = self.conv_P2(features[2])\n",
    "        \n",
    "        # Następnie dopasowujemy rozmiary przestrzenne:\n",
    "        # Zakładamy, że chcemy scalić wszystkie mapy do rozmiaru najdrobniejszej mapy, ale\n",
    "        # często lepszym rozwiązaniem jest dopasowanie do najwyższej rozdzielczości,\n",
    "        # wtedy wykorzystując upsampling.\n",
    "        # Przykład: upsamplujemy p2 do rozmiaru p0:\n",
    "        p2_up = F.interpolate(p2, size=p0.shape[-2:], mode='nearest')\n",
    "        p1_up = F.interpolate(p1, size=p0.shape[-2:], mode='nearest')\n",
    "        \n",
    "        # Teraz łączymy je wzdłuż kanału\n",
    "        fused = torch.cat([p0, p1_up, p2_up], dim=1)  # wynik: [B, 128+256+512, 160, 160]\n",
    "        fused = self.fuse(fused)\n",
    "        \n",
    "        # Ostatecznie przekazujemy scalone cechy do głowicy detekcji\n",
    "        out = self.detect(fused)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef00476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomYOLO(nn.Module):\n",
    "    def __init__(self, pretrained_model, backbone_rgb, backbone_ir):\n",
    "        super().__init__()\n",
    "        self.backbone_rgb = backbone_rgb\n",
    "        self.backbone_ir  = backbone_ir\n",
    "        \n",
    "        self.neck_head = CustomNeck()\n",
    "        \n",
    "    def forward(self, x_rgb, x_ir):\n",
    "        features_rgb = self.backbone_rgb(x_rgb)\n",
    "        features_ir  = self.backbone_ir(x_ir)\n",
    "        \n",
    "        print(features_ir)\n",
    "        print(features_rgb)\n",
    "        fused_features = [f_rgb + f_ir for f_rgb, f_ir in zip(features_rgb, features_ir)]\n",
    "        \n",
    "        # Teraz przekazujemy listę scalonych map do neck/head\n",
    "        # Twój moduł CustomNeck wie już, które mapy co oznaczają\n",
    "        return self.neck_head(fused_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc248f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m x_ir = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m640\u001b[39m, \u001b[32m640\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mCustomYOLO.forward\u001b[39m\u001b[34m(self, x_rgb, x_ir)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_rgb, x_ir):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     features_rgb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     features_ir  = \u001b[38;5;28mself\u001b[39m.backbone_ir(x_ir)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(features_ir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mCustomBackbone.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      8\u001b[39m outputs = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.out_idx:\n\u001b[32m     12\u001b[39m         outputs.append(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:79\u001b[39m, in \u001b[36mConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    Apply convolution, batch normalization and activation to input tensor.\u001b[39;00m\n\u001b[32m     72\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28mself\u001b[39m.bn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "# Inicjalizacja modelu\n",
    "model = CustomYOLO(pretrained_model, backbone_rgb, backbone_ir)\n",
    "\n",
    "x_rgb = torch.randn(1, 3, 640, 640)\n",
    "x_ir = torch.randn(1, 1, 640, 640)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x_rgb, x_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090642a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.0735, -1.8894, -2.3014,  ..., -1.2905, -1.1472, -1.2157],\n",
       "          [-2.0777, -2.0100, -2.1465,  ..., -1.4722, -1.3617, -1.6204],\n",
       "          [-1.3772, -1.3080, -1.4403,  ..., -1.2648, -0.8383, -1.0370],\n",
       "          ...,\n",
       "          [-1.3646, -0.8256, -0.8414,  ..., -1.0123, -1.5207, -1.4113],\n",
       "          [-0.8432, -0.6363, -0.4322,  ..., -1.3918, -0.9484, -1.0619],\n",
       "          [-0.9874, -0.9476, -0.7517,  ..., -1.7655, -1.0977, -0.6837]],\n",
       "\n",
       "         [[ 0.7675,  1.1218, -0.2295,  ..., -0.0980,  0.4910,  0.9436],\n",
       "          [ 0.9526,  0.9408,  0.2694,  ...,  0.2726,  0.5831,  0.6067],\n",
       "          [ 0.6981,  0.8793,  0.1942,  ...,  0.0569,  0.4984,  0.9570],\n",
       "          ...,\n",
       "          [ 0.3456,  0.5022,  0.0493,  ...,  0.4837,  1.0210,  1.3509],\n",
       "          [ 0.2469,  0.2135, -0.7111,  ...,  0.5428,  1.3501,  1.4189],\n",
       "          [ 0.0785,  0.4662, -0.3617,  ...,  0.6158,  1.2786,  1.7035]],\n",
       "\n",
       "         [[ 0.6137,  0.5617,  0.5906,  ...,  0.8256,  0.5149,  0.3238],\n",
       "          [ 0.6676,  0.3449,  0.5414,  ...,  0.0820,  0.0119,  0.2020],\n",
       "          [ 0.2631,  0.1379,  0.1505,  ..., -0.1855, -0.2204, -0.3965],\n",
       "          ...,\n",
       "          [ 0.6050,  0.1217, -0.1021,  ...,  0.3449,  0.8860,  0.8941],\n",
       "          [-0.0135, -0.2805,  0.2700,  ...,  0.8115,  0.4430,  0.6415],\n",
       "          [-0.0347, -0.2413,  0.5029,  ...,  0.9607,  0.7185,  0.6264]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5405,  0.3892,  0.4987,  ...,  1.0601,  0.8559,  1.1057],\n",
       "          [ 0.6754,  0.4745,  0.7047,  ...,  1.6009,  1.0154,  1.1876],\n",
       "          [ 0.3975,  0.2119,  0.3727,  ...,  1.4771,  0.8722,  1.0943],\n",
       "          ...,\n",
       "          [-0.3935, -0.3463, -0.2402,  ...,  1.0427, -0.6815, -0.2627],\n",
       "          [-0.2022, -0.0839, -0.6184,  ...,  0.5521,  0.1191,  0.2107],\n",
       "          [-0.0674, -0.0290, -0.3541,  ...,  0.3000, -0.2036, -0.2186]],\n",
       "\n",
       "         [[-1.3218, -1.6111, -1.1906,  ..., -0.9856, -0.5583, -0.5401],\n",
       "          [-0.9810, -1.1402, -0.8150,  ..., -0.6483, -0.7581, -0.4785],\n",
       "          [ 0.0096,  0.1765,  0.0512,  ...,  0.1393,  0.2596,  0.1153],\n",
       "          ...,\n",
       "          [ 0.0994, -0.0524, -0.0477,  ..., -0.8573,  0.1505, -0.3841],\n",
       "          [-0.9537, -0.7539, -0.7081,  ..., -0.7546, -0.3607, -0.3769],\n",
       "          [-0.7554, -0.6848, -0.7110,  ..., -0.6122, -0.2567, -0.1850]],\n",
       "\n",
       "         [[-1.4237, -1.5225, -1.3342,  ..., -2.6668, -2.2896, -2.5893],\n",
       "          [-1.4524, -1.8094, -1.6739,  ..., -2.5153, -2.2546, -2.2466],\n",
       "          [-1.7961, -1.6727, -1.4084,  ..., -1.6934, -1.8561, -1.6128],\n",
       "          ...,\n",
       "          [-2.2187, -1.9173, -1.3640,  ..., -1.7199, -1.8017, -1.6532],\n",
       "          [-1.9457, -1.6298, -1.3465,  ..., -1.6253, -1.4668, -1.2067],\n",
       "          [-1.9013, -1.9917, -1.4250,  ..., -1.7051, -1.6543, -1.3482]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ee6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class LLVIPDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None, img_size=(640,640)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Ścieżka do katalogu LLVIP.\n",
    "            split (str): 'train' lub 'test'\n",
    "            transform: opcjonalne transformacje (np. albumentations) stosowane na obrazach.\n",
    "            img_size: rozmiar docelowy obrazów (może być potrzebny przy skalowaniu bounding boxów).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Katalogi z obrazami RGB i IR\n",
    "        self.rgb_dir = os.path.join(root_dir, 'visible', split)\n",
    "        self.ir_dir  = os.path.join(root_dir, 'infrared', split)\n",
    "        # Katalog z adnotacjami\n",
    "        self.ann_dir = os.path.join(root_dir, 'Annotations')\n",
    "        \n",
    "        # Pobieramy listę plików; załóżmy, że rozszerzenie to .jpg\n",
    "        self.filenames = [f for f in os.listdir(self.rgb_dir) if f.lower().endswith('.jpg')]\n",
    "        self.filenames.sort()  # Dla spójności\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def parse_voc_annotation(self, xml_file):\n",
    "        \"\"\"\n",
    "        Parsowanie adnotacji w formacie VOC z podanego pliku XML.\n",
    "        Zwraca słownik z:\n",
    "          'boxes': tensor o wymiarach [N, 4] (x_min, y_min, x_max, y_max),\n",
    "          'labels': tensor o wymiarach [N], (przyjmujemy, że etykiety są konwertowane do int)\n",
    "        \n",
    "        Jeśli masz mapowanie nazw klas na liczby, można je dodać.\n",
    "        \"\"\"\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for obj in root.findall('object'):\n",
    "            # Pobierz etykietę jako string\n",
    "            label = obj.find('name').text\n",
    "            # Przykład: przekonwertuj etykietę do liczby – tutaj zakładamy prostą konwersję lub słownik mapowania\n",
    "            # Możesz stworzyć własny słownik, np. {'person': 1, 'car': 2, ...}. Dla przykładu użyjemy hash,\n",
    "            # ale w praktyce lepiej jawnie zdefiniować mapowanie.\n",
    "            label_int = abs(hash(label)) % 1000  # Przykładowa konwersja\n",
    "            \n",
    "            bbox = obj.find('bndbox')\n",
    "            x_min = float(bbox.find('xmin').text)\n",
    "            y_min = float(bbox.find('ymin').text)\n",
    "            x_max = float(bbox.find('xmax').text)\n",
    "            y_max = float(bbox.find('ymax').text)\n",
    "            \n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(label_int)\n",
    "            \n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.zeros((0,4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            \n",
    "        return {'boxes': boxes, 'labels': labels}\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        \n",
    "        # Ścieżki do obrazów oraz adnotacji\n",
    "        rgb_path = os.path.join(self.rgb_dir, filename)\n",
    "        ir_path  = os.path.join(self.ir_dir, filename)\n",
    "        # Zakładamy, że plik XML ma taką samą nazwę, ale z rozszerzeniem .xml\n",
    "        xml_filename = os.path.splitext(filename)[0] + '.xml'\n",
    "        ann_path = os.path.join(self.ann_dir, xml_filename)\n",
    "        \n",
    "        # Wczytanie obrazów\n",
    "        rgb_img = cv2.imread(rgb_path)\n",
    "        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
    "        ir_img = cv2.imread(ir_path, cv2.IMREAD_GRAYSCALE)\n",
    "        ir_img = np.expand_dims(ir_img, axis=-1)  # [H, W, 1]\n",
    "\n",
    "        # Opcjonalna zmiana rozmiaru\n",
    "        rgb_img = cv2.resize(rgb_img, self.img_size)\n",
    "        ir_img = cv2.resize(ir_img, self.img_size)\n",
    "        \n",
    "        # Normalizacja\n",
    "        rgb_img = rgb_img.astype(np.float32) / 255.0\n",
    "        ir_img = ir_img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Konwersja na tensory: zmiana wymiarów: H, W, C -> C, H, W\n",
    "        rgb_img = torch.from_numpy(rgb_img).permute(2, 0, 1)\n",
    "        ir_img = torch.from_numpy(ir_img).permute(2, 0, 1)\n",
    "        \n",
    "        # Wczytanie adnotacji\n",
    "        target = self.parse_voc_annotation(ann_path)\n",
    "        \n",
    "        # Jeżeli używasz dodatkowych transformacji (np. augumentacji) to możesz je zastosować tu\n",
    "        if self.transform is not None:\n",
    "            # transform powinien obsługiwać dwa obrazy. Możesz zdefiniować własną funkcję, która będzie to obsługiwać.\n",
    "            augmented = self.transform(image=rgb_img.numpy(), mask=ir_img.numpy())\n",
    "            rgb_img = torch.from_numpy(augmented['image']).permute(2, 0, 1)\n",
    "            ir_img = torch.from_numpy(augmented['mask']).permute(2, 0, 1)\n",
    "        \n",
    "        return rgb_img, ir_img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f51de76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja collate_fn do DataLoadera (gdy targets mają zmienną liczbę elementów)\n",
    "def collate_fn(batch):\n",
    "    rgb_imgs, ir_imgs, targets = zip(*batch)\n",
    "    rgb_imgs = torch.stack(rgb_imgs, dim=0)\n",
    "    ir_imgs  = torch.stack(ir_imgs, dim=0)\n",
    "    return rgb_imgs, ir_imgs, list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d5946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_loss(outputs, targets):\n",
    "    \"\"\"\n",
    "    Funkcja straty tylko do celów demonstracyjnych.\n",
    "    Zakładamy, że outputs to tensor wyjściowy modelu,\n",
    "    a targets to lista adnotacji – tutaj generujemy cel jako tensor zer.\n",
    "    \"\"\"\n",
    "    # Przykladowo celowy tensor o takim samym kształcie jak outputs\n",
    "    target_tensor = torch.zeros_like(outputs)\n",
    "    loss = F.mse_loss(outputs, target_tensor)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6101ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, num_epochs=10, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (x_rgb, x_ir, targets) in enumerate(train_loader):\n",
    "            x_rgb = x_rgb.to(device)\n",
    "            x_ir  = x_ir.to(device)\n",
    "            # Przenoszenie adnotacji (targets) na urządzenie\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_rgb, x_ir)\n",
    "            loss = dummy_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0741d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e909513",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Uruchomienie treningu\u001b[39;00m\n\u001b[32m     18\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, optimizer, num_epochs, device)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m      5\u001b[39m     epoch_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_rgb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_rgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_ir\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal-YOLO\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mLLVIPDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Konwersja na tensory: zmiana wymiarów: H, W, C -> C, H, W\u001b[39;00m\n\u001b[32m    101\u001b[39m rgb_img = torch.from_numpy(rgb_img).permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m ir_img = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mir_img\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Wczytanie adnotacji\u001b[39;00m\n\u001b[32m    105\u001b[39m target = \u001b[38;5;28mself\u001b[39m.parse_voc_annotation(ann_path)\n",
      "\u001b[31mRuntimeError\u001b[39m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "# Ścieżka do katalogu LLVIP – zmodyfikuj na właściwą ścieżkę\n",
    "root_dir = 'LLVIP'\n",
    "\n",
    "# Utworzenie datasetu i DataLoadera (dla treningu)\n",
    "dataset = LLVIPDataset(root_dir, split='train', img_size=(640, 640))\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Wczytanie pretrenowanego modelu YOLOv8m (upewnij się, że model 'yolov8m.pt' jest dostępny)\n",
    "pretrained_model = YOLO('yolov8m.pt')\n",
    "\n",
    "# Utworzenie customowego modelu\n",
    "model = CustomYOLO(pretrained_model, backbone_rgb, backbone_ir)\n",
    "\n",
    "# Konfiguracja optymalizatora\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Uruchomienie treningu\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, optimizer, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
