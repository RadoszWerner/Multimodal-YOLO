{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from xml.etree import ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ≈öcie≈ºli datasetu\n",
    "DATASET_PATH = \"LLVIP\"\n",
    "IMG_V_PATH = os.path.join(DATASET_PATH, \"visible\", \"train\")\n",
    "IMG_IR_PATH = os.path.join(DATASET_PATH, \"infrared\", \"train\")\n",
    "ANNOTATIONS_PATH = os.path.join(DATASET_PATH, \"Annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformacje dla YOLO\n",
    "wymagany input i normalizacja do zakresu [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funckje do konwersji danych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def parse_voc_annotation(xml_path):\n",
    "    \"\"\"Parsuje plik XML z adnotacjami w formacie VOC\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    labels = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        label = obj.find(\"name\").text\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        xmin = int(bbox.find(\"xmin\").text)\n",
    "        ymin = int(bbox.find(\"ymin\").text)\n",
    "        xmax = int(bbox.find(\"xmax\").text)\n",
    "        ymax = int(bbox.find(\"ymax\").text)\n",
    "        labels.append((label, xmin, ymin, xmax, ymax))\n",
    "\n",
    "    return labels\n",
    "    \n",
    "def load_image_pairs(img_name, dataset_path):\n",
    "    \"\"\" Wczytuje parƒô obraz√≥w RGB i IR, skaluje i ≈ÇƒÖczy w jeden tensor. \"\"\"\n",
    "    img_rgb = Image.open(os.path.join(dataset_path, \"visible/train\", img_name)).convert(\"RGB\")\n",
    "    img_ir = Image.open(os.path.join(dataset_path, \"infrared/train\", img_name)).convert(\"L\")  # Skala szaro≈õci\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((416, 416)),  # Przeskalowanie do wymiar√≥w YOLO\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img_rgb = transform(img_rgb)  # (3, 416, 416)\n",
    "    img_ir = transform(img_ir)  # (1, 416, 416)\n",
    "\n",
    "    img_fused = torch.cat((img_rgb, img_ir), dim=0)  # (4, 416, 416)\n",
    "    return img_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiar obrazu po fuzji: torch.Size([4, 416, 416])\n",
      "Adnotacje: [('person', 287, 428, 351, 662), ('person', 351, 391, 424, 642), ('person', 466, 367, 550, 614), ('person', 700, 354, 761, 585), ('person', 704, 517, 806, 794), ('person', 1124, 22, 1196, 245)]\n"
     ]
    }
   ],
   "source": [
    "# Test: Wczytanie jednej pr√≥bki\n",
    "sample_name = \"010001.jpg\"  # Nazwa przyk≈Çadowego obrazu\n",
    "img_fused = load_image_pairs(sample_name, DATASET_PATH)\n",
    "annotations = parse_voc_annotation(os.path.join(ANNOTATIONS_PATH, \"010001.xml\"))\n",
    "\n",
    "print(f\"Rozmiar obrazu po fuzji: {img_fused.shape}\")  # Powinno byƒá (4, 416, 416)\n",
    "print(f\"Adnotacje: {annotations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ≈ÅƒÖczenie obraz√≥w w obraz 4-ro kana≈Çowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Przetwarzanie train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12025/12025 [06:06<00:00, 32.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Zapisano obrazy do datasets/LLVIP_fused\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Przetwarzanie test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3463/3463 [01:44<00:00, 33.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Zapisano obrazy do datasets/LLVIP_fused\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from torchvision import transforms\n",
    "# from tqdm import tqdm  # Pasek postƒôpu\n",
    "\n",
    "# # ≈öcie≈ºki\n",
    "# DATASET_PATH = \"LLVIP\"\n",
    "# SAVE_PATH = \"datasets/LLVIP_fused\"\n",
    "\n",
    "# # Transformacja obrazu do formatu PyTorch\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),  # Zamiana na tensor (0,1)\n",
    "#     transforms.Resize((416, 416))  # Resize do 416x416\n",
    "# ])\n",
    "\n",
    "# # Funkcja do przetwarzania i zapisu obraz√≥w\n",
    "# def process_and_save_images(split):\n",
    "#     input_v_path = os.path.join(DATASET_PATH, \"visible\", split)\n",
    "#     input_ir_path = os.path.join(DATASET_PATH, \"infrared\", split)\n",
    "#     output_path = os.path.join(SAVE_PATH, split)\n",
    "\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "#     image_list = sorted(os.listdir(input_v_path))\n",
    "\n",
    "#     for img_name in tqdm(image_list, desc=f\"Przetwarzanie {split}\"):\n",
    "#         # Wczytanie obraz√≥w RGB i IR\n",
    "#         img_v_path = os.path.join(input_v_path, img_name)\n",
    "#         img_ir_path = os.path.join(input_ir_path, img_name)\n",
    "\n",
    "#         img_v = cv2.imread(img_v_path)  # RGB (3 kana≈Çy)\n",
    "#         img_ir = cv2.imread(img_ir_path, cv2.IMREAD_GRAYSCALE)  # IR (1 kana≈Ç)\n",
    "\n",
    "#         if img_v is None or img_ir is None:\n",
    "#             print(f\"POMINIƒòTO {img_name} (brak pliku)\")\n",
    "#             continue\n",
    "\n",
    "#         # Konwersja do tensora\n",
    "#         img_v = transform(img_v)\n",
    "#         img_ir = transform(img_ir).squeeze(0)  # Usuniƒôcie zbƒôdnego wymiaru\n",
    "\n",
    "#         # ≈ÅƒÖczenie jako (RGB + IR) ‚Üí 4 kana≈Çy\n",
    "#         img_fused = torch.cat((img_v, img_ir.unsqueeze(0)), dim=0)\n",
    "\n",
    "\n",
    "#         # Konwersja do NumPy\n",
    "#         img_fused_np = (img_fused.numpy() * 255).astype(np.uint8)  # (4, 416, 416)\n",
    "\n",
    "#         # Zamiana kolejno≈õci osi do OpenCV: (C, H, W) ‚Üí (H, W, C)\n",
    "#         img_fused_np = np.transpose(img_fused_np, (1, 2, 0))\n",
    "\n",
    "#         # **Zapis do formatu obs≈ÇugujƒÖcego 4 kana≈Çy (PNG)**\n",
    "#         save_img_path = os.path.join(output_path, img_name.replace(\".jpg\", \".tif\"))\n",
    "#         cv2.imwrite(save_img_path, img_fused_np)\n",
    "\n",
    "#         # **Sprawdzenie po zapisie**\n",
    "#         img_check = cv2.imread(save_img_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "#     print(f\"Zapisano obrazy do {output_path}\")\n",
    "\n",
    "# # Przetwarzanie zbior√≥w train i test\n",
    "# process_and_save_images(\"train\")\n",
    "# process_and_save_images(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kod do konwersji adnotacji z formatu VOC do YOLO\n",
    "\n",
    "# import os\n",
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# # ≈öcie≈ºki do katalog√≥w\n",
    "# ANNOTATIONS_PATH = \"LLVIP/Annotations\"\n",
    "# DATASET_PATH = \"datasets/LLVIP_fused/train\"  # Tylko dla zbioru treningowego\n",
    "\n",
    "# # Pobranie listy klas\n",
    "# CLASSES = [\"person\"]  # Mo≈ºesz dodaƒá wiƒôcej klas\n",
    "\n",
    "# # Funkcja konwertujƒÖca XML do formatu YOLO\n",
    "# def convert_voc_to_yolo(xml_file, output_txt):\n",
    "#     tree = ET.parse(xml_file)\n",
    "#     root = tree.getroot()\n",
    "\n",
    "#     img_width = int(root.find(\"size/width\").text)\n",
    "#     img_height = int(root.find(\"size/height\").text)\n",
    "\n",
    "#     with open(output_txt, \"w\") as f:\n",
    "#         for obj in root.findall(\"object\"):\n",
    "#             class_name = obj.find(\"name\").text\n",
    "#             if class_name not in CLASSES:\n",
    "#                 continue  # Pomijamy nieznane klasy\n",
    "\n",
    "#             class_id = CLASSES.index(class_name)\n",
    "\n",
    "#             bbox = obj.find(\"bndbox\")\n",
    "#             xmin = int(bbox.find(\"xmin\").text)\n",
    "#             ymin = int(bbox.find(\"ymin\").text)\n",
    "#             xmax = int(bbox.find(\"xmax\").text)\n",
    "#             ymax = int(bbox.find(\"ymax\").text)\n",
    "\n",
    "#             # YOLO format (normalizacja do [0,1])\n",
    "#             x_center = (xmin + xmax) / (2 * img_width)\n",
    "#             y_center = (ymin + ymax) / (2 * img_height)\n",
    "#             bbox_width = (xmax - xmin) / img_width\n",
    "#             bbox_height = (ymax - ymin) / img_height\n",
    "\n",
    "#             f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
    "\n",
    "# # Przetwarzanie zbioru treningowego\n",
    "# for img_file in os.listdir(DATASET_PATH):\n",
    "#     if img_file.endswith(\".tif\") or img_file.endswith(\".jpg\"):  # Sprawdzamy tylko obrazy\n",
    "#         base_name = os.path.splitext(img_file)[0]  # Usuwamy rozszerzenie\n",
    "#         xml_path = os.path.join(ANNOTATIONS_PATH, f\"{base_name}.xml\")\n",
    "#         txt_path = os.path.join(DATASET_PATH, f\"{base_name}.txt\")\n",
    "#         print(f\"üîç Sprawdzam: {img_file} ‚Üí {xml_path}\")  # DODANE LOGOWANIE\n",
    "\n",
    "#         if os.path.exists(xml_path):\n",
    "#             convert_voc_to_yolo(xml_path, txt_path)\n",
    "#             print(f\"‚úÖ Przetworzono: {img_file} ‚Üí {base_name}.txt\")\n",
    "#         else:\n",
    "#             print(f\"‚ö†Ô∏è Brak anotacji dla {img_file} (oczekiwano {xml_path})\")\n",
    "\n",
    "# print(\"üéØ Konwersja zako≈Ñczona!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wszystkie obrazy w train majƒÖ etykiety!\n",
      "Wszystkie obrazy w test majƒÖ etykiety!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATASET_PATH = \"datasets/LLVIP_fused\"\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    missing_labels = []\n",
    "    img_folder = os.path.join(DATASET_PATH, split)\n",
    "\n",
    "    for img_file in os.listdir(img_folder):\n",
    "        if img_file.endswith(\".png\"):\n",
    "            txt_file = img_file.replace(\".png\", \".txt\")\n",
    "            if not os.path.exists(os.path.join(img_folder, txt_file)):\n",
    "                missing_labels.append(img_file)\n",
    "\n",
    "    if missing_labels:\n",
    "        print(f\"Brakuje etykiet dla {len(missing_labels)} obraz√≥w w {split}:\")\n",
    "        print(missing_labels[:10])  # Poka≈ºemy pierwsze 10 brakujƒÖcych\n",
    "    else:\n",
    "        print(f\"Wszystkie obrazy w {split} majƒÖ etykiety!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dostosowanie modelu pod obraz 4-ro kana≈Çowy\n",
    "1. Zmiana 1 warstwy\n",
    "2. wymuszenie obslugi 4 kana≈Çow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nowa pierwsza warstwa: Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Wczytanie modelu\n",
    "model3 = YOLO(\"yolov3-sppu.pt\").to(\"cuda\")  # Przenosimy ca≈Çy model na GPU\n",
    "\n",
    "# Pobranie pierwszej warstwy konwolucyjnej\n",
    "conv1 = model3.model.model[0].conv  \n",
    "\n",
    "# Nowa warstwa\n",
    "new_conv1 = nn.Conv2d(\n",
    "    in_channels=4, \n",
    "    out_channels=conv1.out_channels, \n",
    "    kernel_size=conv1.kernel_size, \n",
    "    stride=conv1.stride, \n",
    "    padding=conv1.padding, \n",
    "    bias=conv1.bias is not None\n",
    ")\n",
    "\n",
    "# Kopiowanie wag RGB, IR losowo\n",
    "with torch.no_grad():\n",
    "    new_conv1.weight[:, :3] = conv1.weight\n",
    "    new_conv1.weight[:, 3] = torch.randn_like(conv1.weight[:, 0]) * 0.01  # IR losowe wagi\n",
    "\n",
    "model3.model.model[0].conv = new_conv1.to(\"cuda\")\n",
    "\n",
    "model3.overrides['imgsz'] = 416  # Rozmiar wej≈õcia\n",
    "model3.overrides['augment'] = True  # W≈ÇƒÖcz augmentacjƒô\n",
    "model3.save(\"yolov3-4ch.pt\")\n",
    "# Sprawdzenie poprawno≈õci\n",
    "print(\"Nowa pierwsza warstwa:\", model3.model.model[0].conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.94  Python-3.12.9 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov3-4ch.pt, data=llvip.yaml, epochs=10, time=None, patience=100, batch=8, imgsz=416, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train7\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 1]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     37056  ultralytics.nn.modules.block.Bottleneck      [64, 64]                      \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    295680  ultralytics.nn.modules.block.Bottleneck      [128, 128]                    \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  8   4724736  ultralytics.nn.modules.block.Bottleneck      [256, 256]                    \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  8  18886656  ultralytics.nn.modules.block.Bottleneck      [512, 512]                    \n",
      "  9                  -1  1   4720640  ultralytics.nn.modules.conv.Conv             [512, 1024, 3, 2]             \n",
      " 10                  -1  4  37761024  ultralytics.nn.modules.block.Bottleneck      [1024, 1024]                  \n",
      " 11                  -1  1   9440256  ultralytics.nn.modules.block.Bottleneck      [1024, 1024, False]           \n",
      " 12                  -1  1   1574912  ultralytics.nn.modules.block.SPP             [1024, 512, [5, 9, 13]]       \n",
      " 13                  -1  1   4720640  ultralytics.nn.modules.conv.Conv             [512, 1024, 3, 1]             \n",
      " 14                  -1  1    525312  ultralytics.nn.modules.conv.Conv             [1024, 512, 1, 1]             \n",
      " 15                  -1  1   4720640  ultralytics.nn.modules.conv.Conv             [512, 1024, 3, 1]             \n",
      " 16                  -2  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 18             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   2950656  ultralytics.nn.modules.block.Bottleneck      [768, 512, False]             \n",
      " 20                  -1  1   2360832  ultralytics.nn.modules.block.Bottleneck      [512, 512, False]             \n",
      " 21                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 22                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 1]              \n",
      " 23                  -2  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 24                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 25             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 26                  -1  1    738048  ultralytics.nn.modules.block.Bottleneck      [384, 256, False]             \n",
      " 27                  -1  2   1181184  ultralytics.nn.modules.block.Bottleneck      [256, 256, False]             \n",
      " 28        [27, 22, 15]  1   7058131  ultralytics.nn.modules.head.Detect           [1, [256, 512, 1024]]         \n",
      "YOLOv3-spp summary: 185 layers, 104,742,835 parameters, 104,742,819 gradients, 283.8 GFLOPs\n",
      "\n",
      "Transferred 516/523 items from pretrained weights\n",
      "Freezing layer 'model.28.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Projects\\multimodal\\datasets\\LLVIP_fused\\train.cache... 12025 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12025/12025 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Projects\\multimodal\\datasets\\LLVIP_fused\\test.cache... 3463 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3463/3463 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train7\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 85 weight(decay=0.0), 92 weight(decay=0.0005), 91 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train7\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      4.72G       2.02      1.575      1.745          4        416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [06:04<00:00,  4.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:43<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302      0.578      0.549       0.56      0.269\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      5.47G      1.896       1.33      1.648          1        416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [05:39<00:00,  4.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:43<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302      0.822      0.743      0.806      0.421\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      5.76G      1.811      1.175      1.587          4        416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [05:33<00:00,  4.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:42<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302      0.842      0.666      0.767      0.407\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      5.89G      1.759      1.078      1.551          1        416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [05:15<00:00,  4.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:40<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302      0.874      0.708      0.817      0.437\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      6.37G      1.705     0.9886      1.512          2        416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [05:15<00:00,  4.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:40<00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302      0.881      0.762      0.839      0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      6.37G      1.654     0.9151      1.481          1        416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [05:15<00:00,  4.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:40<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302      0.886      0.793      0.868      0.494\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      6.37G      1.615     0.8644      1.456          1        416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [05:15<00:00,  4.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:43<00:00,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302      0.918      0.788      0.869      0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      6.37G      1.571     0.8112      1.429          6        416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [05:32<00:00,  4.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:42<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302      0.878      0.825      0.888      0.504\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      6.37G      1.529     0.7691      1.408          3        416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [05:31<00:00,  4.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:42<00:00,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302      0.864       0.82      0.883      0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      6.37G      1.491     0.7278      1.383          3        416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1504/1504 [05:31<00:00,  4.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:43<00:00,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302       0.87       0.84       0.89      0.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 1.048 hours.\n",
      "Optimizer stripped from runs\\detect\\train7\\weights\\last.pt, 209.8MB\n",
      "Optimizer stripped from runs\\detect\\train7\\weights\\best.pt, 209.8MB\n",
      "\n",
      "Validating runs\\detect\\train7\\weights\\best.pt...\n",
      "Ultralytics 8.3.94  Python-3.12.9 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "YOLOv3-spp summary (fused): 100 layers, 104,714,099 parameters, 0 gradients, 283.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:42<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3463       8302      0.871      0.839       0.89      0.524\n",
      "Speed: 0.1ms preprocess, 7.4ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train7\u001b[0m\n",
      "First layer weights shape: torch.Size([32, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Funkcja do sprawdzania wymiar√≥w wag\n",
    "def check_conv_weights():\n",
    "    w = model3.model.model[0].conv.weight\n",
    "    print(f\"First layer weights shape: {w.shape}\")  # Powinno byƒá (out_channels, 4, kernel_size, kernel_size)\n",
    "\n",
    "# Wczytanie modelu po modyfikacji, ≈ºeby uniknƒÖƒá resetowania przy ka≈ºdym `train()`\n",
    "model3 = YOLO(\"yolov3-4ch.pt\").to(\"cuda\")\n",
    "\n",
    "# Trenowanie modelu YOLOv3 na 4-kana≈Çowych obrazach\n",
    "model3.train(data=\"llvip.yaml\", epochs=10, imgsz=416, batch=8, device=\"cuda\")\n",
    "\n",
    "# Sprawdzenie po zako≈Ñczeniu treningu\n",
    "check_conv_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1mchannels\u001b[0m' is not a valid YOLO argument. \n\n    Arguments received: ['yolo', '--f=c:\\\\Users\\\\Radosz\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3104cfdbea569844c11f4ab8cce28fc55a88dea40.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of frozenset({'obb', 'segment', 'classify', 'detect', 'pose'})\n                MODE (required) is one of frozenset({'predict', 'val', 'benchmark', 'export', 'train', 'track'})\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video/file.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92md:\\Projects\\multimodal\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3549\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Cell \u001b[92mIn[76]\u001b[39m\u001b[92m, line 4\u001b[39m\n    model3.train(data=\"llvip.yaml\", epochs=25, imgsz=416, batch=8, device=\"cuda\")\n",
      "  File \u001b[92md:\\Projects\\multimodal\\venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:785\u001b[39m in \u001b[95mtrain\u001b[39m\n    self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\n",
      "  File \u001b[92md:\\Projects\\multimodal\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:105\u001b[39m in \u001b[95m__init__\u001b[39m\n    self.args = get_cfg(cfg, overrides)\n",
      "  File \u001b[92md:\\Projects\\multimodal\\venv\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py:310\u001b[39m in \u001b[95mget_cfg\u001b[39m\n    check_dict_alignment(cfg, overrides)\n",
      "\u001b[36m  \u001b[39m\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal\\venv\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py:497\u001b[39m\u001b[36m in \u001b[39m\u001b[35mcheck_dict_alignment\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mraise SyntaxError(string + CLI_HELP_MSG) from e\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>\u001b[39m\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m '\u001b[31m\u001b[1mchannels\u001b[0m' is not a valid YOLO argument. \n\n    Arguments received: ['yolo', '--f=c:\\\\Users\\\\Radosz\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3104cfdbea569844c11f4ab8cce28fc55a88dea40.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of frozenset({'obb', 'segment', 'classify', 'detect', 'pose'})\n                MODE (required) is one of frozenset({'predict', 'val', 'benchmark', 'export', 'train', 'track'})\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video/file.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "# Trenowanie modelu YOLOv3 na 4-kana≈Çowych obrazach\n",
    "\n",
    "\n",
    "model3.train(data=\"llvip.yaml\", epochs=25, imgsz=416, batch=8, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Za≈Çaduj model\n",
    "model11 = YOLO(\"yolo11n.pt\").to(\"cuda\")\n",
    "\n",
    "# Pobierz pierwszƒÖ warstwƒô konwolucyjnƒÖ\n",
    "conv1 = model.model.model[0].conv\n",
    "\n",
    "# Nowa warstwa 4-kana≈Çowa\n",
    "new_conv1 = nn.Conv2d(\n",
    "    in_channels=4, \n",
    "    out_channels=conv1.out_channels, \n",
    "    kernel_size=conv1.kernel_size, \n",
    "    stride=conv1.stride, \n",
    "    padding=conv1.padding, \n",
    "    bias=conv1.bias is not None\n",
    ")\n",
    "\n",
    "# Kopiowanie wag RGB, a kana≈Ç IR inicjalizowany losowo\n",
    "with torch.no_grad():\n",
    "    new_conv1.weight[:, :3] = conv1.weight  # Skopiuj RGB\n",
    "    new_conv1.weight[:, 3] = torch.randn_like(conv1.weight[:, 0]) * 0.01  # Losowe warto≈õci dla IR\n",
    "\n",
    "# Podstawienie nowej warstwy\n",
    "model11.model.model[0].conv = new_conv1.to(\"cuda\")\n",
    "\n",
    "# Sprawdzenie\n",
    "print(model.model.model[0].conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics.data import YOLODataset\n",
    "\n",
    "class CustomYOLODataset(YOLODataset):\n",
    "    def load_image(self, i):\n",
    "        path = self.im_files[i]\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # Wczytanie 4-kana≈Çowego obrazu\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Nie mo≈ºna wczytaƒá obrazu: {path}\")\n",
    "        return img, cv2.imread(path).shape[:2]  # Zachowanie oryginalnych wymiar√≥w\n",
    "\n",
    "from ultralytics.engine.trainer import BaseTrainer\n",
    "\n",
    "class CustomTrainer(BaseTrainer):\n",
    "    def get_dataloader(self, dataset_path, batch_size, img_size, augment):\n",
    "        return CustomYOLODataset(dataset_path, img_size, batch_size, augment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.94  Python-3.12.9 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8191MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=llvip.yaml, epochs=50, time=None, patience=100, batch=8, imgsz=416, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train3\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "This task trainer doesn't support loading cfg files",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCustomTrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllvip.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m416\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal\\venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:787\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = (trainer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smart_load(\u001b[33m\"\u001b[39m\u001b[33mtrainer\u001b[39m\u001b[33m\"\u001b[39m))(overrides=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43myaml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    790\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\multimodal\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:635\u001b[39m, in \u001b[36mBaseTrainer.get_model\u001b[39m\u001b[34m(self, cfg, weights, verbose)\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg=\u001b[38;5;28;01mNone\u001b[39;00m, weights=\u001b[38;5;28;01mNone\u001b[39;00m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    634\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get model and raise NotImplementedError for loading cfg files.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThis task trainer doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt support loading cfg files\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: This task trainer doesn't support loading cfg files"
     ]
    }
   ],
   "source": [
    "model11.train(trainer=CustomTrainer, data=\"llvip.yaml\", epochs=50, imgsz=416, batch=8, device=\"cuda\", verbose=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
